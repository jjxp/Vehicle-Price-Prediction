{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da790868-5f66-4db2-ac40-6d6a6d3909ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import category_encoders as ce\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc75fe0-d712-4cce-a91a-05669244e90e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initial data preparation\n",
    "In this step, we'll read the source data and see what we've collected in the previous step.\n",
    "We will also perform some transformations (mainly column datatype casting) to clean the source data before delving into the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa5e7d4-a1eb-4265-9683-fca76636eb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "X = pd.read_csv('vehicle_listing_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046914ae-04c4-4a0d-8e29-33eeccb07de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a first look at what we've got\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066a5873-d7dd-45e6-924e-5a2d39c9b929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop first index column\n",
    "X.drop(columns = X.columns[0], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde95c75-e026-4cb9-9c2f-61fc39f3df38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is, particulary, one column with wrong values\n",
    "X['seller'].unique()\n",
    "\n",
    "# Let's see if this is a situation that we should handle in the scraping step, or there's just a few rows which values\n",
    "# differ from the expected ones\n",
    "X[(X.seller != 'Profesional') & (X.seller != 'Ver fotos')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a54fb3-f03d-4d77-a58c-2216cb6e29c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# There are only 13 rows where the seller is mistaken in the dataset. Drop them.\n",
    "X.drop(X[(X.seller != 'Profesional') & (X.seller != 'Ver fotos')].index, inplace = True)\n",
    "\n",
    "# It's not going to make a difference, but to keep the data clean, let's change 'Ver fotos' with the other\n",
    "# expected value rather than Professional: 'Particular'\n",
    "def clean_seller(row):\n",
    "    '''\n",
    "    Checks the value of the Seller column for a given row and returns whether it's a 'Particular' or a 'Profesional' seller\n",
    "    '''\n",
    "    if row.seller == 'Ver fotos':\n",
    "        return 'Particular'\n",
    "    return row.seller\n",
    "\n",
    "X['seller'] = X.apply(lambda row: clean_seller(row), axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45300973-f290-4992-8056-f1b67371cf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_brand_model(row):\n",
    "    '''\n",
    "    Gets the Brand and Model for a given row, using information from the Title column\n",
    "    '''\n",
    "    tokenized = row.title.split(' - ')\n",
    "    if len(tokenized) == 1:\n",
    "        tokenized = row.title.split(' ')\n",
    "        brand = tokenized[0]\n",
    "        model = tokenized[1]\n",
    "    else:\n",
    "        brand = tokenized[0]\n",
    "        model = ' '.join(tokenized[1].split(' ')[0:1])\n",
    "    \n",
    "    return (brand, model)\n",
    "\n",
    "def clean_currency(x):\n",
    "    '''\n",
    "    If the given value is a string, returns a clean version without currency symbols or radix characters\n",
    "    '''\n",
    "    if isinstance(x, str):\n",
    "        return(x.replace('â‚¬', '').replace('.', ''))\n",
    "    return x\n",
    "\n",
    "def clean_mileage(x):\n",
    "    '''\n",
    "    If the given instance is a string, returns a clean version removing the 'kms' suffix and radix characters.\n",
    "    If a null value is given, return 0.\n",
    "    '''\n",
    "    if isinstance(x, str):\n",
    "        return(x.replace('kms', '').replace('.', ''))\n",
    "    if x is None:\n",
    "        return 0\n",
    "    return x\n",
    "\n",
    "def clean_hp(x):\n",
    "    '''\n",
    "    If the given instance is a string, returns a clean version removing the 'CV' suffix.\n",
    "    If a null value is given, return 0.\n",
    "    '''\n",
    "    if isinstance(x, str):\n",
    "        return(x.replace('CV', ''))\n",
    "    if x is None:\n",
    "        return 0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69acf5b5-6655-466c-aef5-c695da699bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations to obtain the brand and model for each vehicle\n",
    "X['brand'] = X.apply(lambda row: get_brand_model(row)[0], axis=1)\n",
    "X['model'] = X.apply(lambda row: get_brand_model(row)[1], axis=1)\n",
    "\n",
    "# Apply transformations to clean the price, mileage and horse power columns. Eventually, cast them to integer\n",
    "X.price = X.price.apply(clean_currency).astype('int')\n",
    "X.mileage = X.mileage.apply(clean_mileage).astype('int')\n",
    "X.hp = X.hp.apply(clean_hp).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ed2083-eb73-466e-8029-f4603f410e66",
   "metadata": {},
   "source": [
    "## Data preparation for machine learning models\n",
    "In this step, we'll continue working with the data that we've just transformed.\n",
    "Now, we should have a dataset with numerical values where expected and with readable string formats.\n",
    "However, we still can have null or missing values and the data needs to be prepared for Machine Learning.\n",
    "\n",
    "In this section we will separate training data from validation data. We will also perform preprocessing tasks for both numerical and categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c7797a-61f8-4191-a66c-2b8cb2783517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows in which the target data is missing\n",
    "X.dropna(axis=0, subset=['price'], inplace=True)\n",
    "\n",
    "# Separate the target data into a Pandas Series and drop target and features that are not in our interest in the original dataset\n",
    "y = X.price\n",
    "X.drop(['price', 'title', 'url', 'desc'], axis=1, inplace=True)\n",
    "\n",
    "# Break off the data into training and validation dataframes\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9a006e-991b-44c0-ab29-c7dcb90f0c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the categorical columns and separate them into low and high cardinality lists. The threshold is set in 15 unique values.\n",
    "categorical_cols = [cname for cname in X_train.columns if X_train[cname].dtype == \"object\"]\n",
    "\n",
    "low_cardinality_cols = [cname for cname in categorical_cols if X_train[cname].nunique() < 15]\n",
    "high_cardinality_cols = [cname for cname in categorical_cols if X_train[cname].nunique() >= 15]\n",
    "\n",
    "# Select numerical columns\n",
    "numeric_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ('int', 'int32', 'int64')]\n",
    "\n",
    "# Check that we're obtaining the expected results\n",
    "print('# Low cardinality columns: ', low_cardinality_cols)\n",
    "print('# High cardinality columns: ', high_cardinality_cols)\n",
    "print('# Numerical columns: ', numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ca1434-8e51-4c20-ba46-f83d536dd754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for numerical data\n",
    "# We will imput the mean values for missing values in numerical columns\n",
    "numerical_transformer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "# We will use the most frequent values for missing values in categorical columns with low cardinality and one-hot encode them\n",
    "onehot_categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', ce.OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# We will also use the most frequent values for missing values in categorical columns with high cardinality and hash encode them\n",
    "hashing_categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('hashing', ce.HashingEncoder())\n",
    "])\n",
    "\n",
    "# Bundle all the previous transformers together\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numeric_cols),\n",
    "        ('oh_cat', onehot_categorical_transformer, low_cardinality_cols),\n",
    "        ('hash_cat', hashing_categorical_transformer, high_cardinality_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26b1873-785c-4c75-aabd-347498874f65",
   "metadata": {},
   "source": [
    "## Machine Learning algorithms\n",
    "Now, our data is finally ready and we'll create a model using different supervised Machine Learning algorithms and we will compare the mean absolute error that we obtain from each of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff39812-c137-4cd9-8705-8ea3f25be170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Cross Gradient Regressor model\n",
    "# We're setting 1000 estimators, a learning rate of 0.05 and 4 jobs to run in paralel\n",
    "model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\n",
    "\n",
    "# Create a pipeline that will perform all the preprocessing steps when fitting our datasets\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)\n",
    "                             ])\n",
    "\n",
    "# Fit model for training data\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1915733d-932b-4305-904f-dc789720c25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform predictions over the validation data\n",
    "preditions = pipeline.predict(X_valid)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_valid, preditions)\n",
    "print('MAE: ', mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc6bdd5-3615-42b9-b13d-7edad05d6286",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = -1 * cross_val_score(pipeline, X, y,\n",
    "                              cv=5,\n",
    "                              scoring='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7de470-a632-4904-bbb0-67bb66b91152",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
